{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3842332,"sourceType":"datasetVersion","datasetId":2286778},{"sourceId":4828009,"sourceType":"datasetVersion","datasetId":2797203},{"sourceId":6358196,"sourceType":"datasetVersion","datasetId":3579787},{"sourceId":7861524,"sourceType":"datasetVersion","datasetId":4611726},{"sourceId":7861579,"sourceType":"datasetVersion","datasetId":4611768},{"sourceId":8273774,"sourceType":"datasetVersion","datasetId":4912750}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"pip install noisereduce","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport librosa\nimport os\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Lambda, Dropout, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nimport noisereduce as nr\nfrom sklearn.metrics import accuracy_score, roc_curve, auc, f1_score\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pre-proccessing","metadata":{}},{"cell_type":"code","source":"\nDATASET_PATH = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_train/flac\"\nLABEL_FILE_PATH = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\"\nSAMPLE_RATE = 16000\nDURATION = 5\nN_MFCC = 13  # Typical number of MFCC features\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_and_transform_audio_to_mfcc(filepath, target_duration, sample_rate, n_mfcc, n_fft=2048, hop_length=512, time_steps=109):\n    try:\n        audio, sr = librosa.load(filepath, sr=sample_rate, duration=target_duration)\n        \n        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n        \n        if mfccs.shape[1] < time_steps:\n            mfccs = np.pad(mfccs, ((0, 0), (0, time_steps - mfccs.shape[1])), 'constant')\n        elif mfccs.shape[1] > time_steps:\n            mfccs = mfccs[:, :time_steps]\n        \n        return mfccs.reshape((n_mfcc, time_steps, 1))\n    except Exception as e:\n        print(f\"Error processing {filepath}: {str(e)}\")\n        return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"code","source":"\n\n\n# Initialize the dictionary to hold the first bonafide audio for each speaker\nspeaker_refs = {}\n\ntry:\n    with open(LABEL_FILE_PATH, 'r') as label_file:\n        lines = label_file.readlines()\nexcept IOError:\n    print(f\"Could not read file: {LABEL_FILE_PATH}\")\n\n# First pass to fill speaker_refs with the first bonafide audio for each speaker\nfor line in lines:\n    parts = line.strip().split()\n    speaker_id, audio_filename, label = parts[0], parts[1], parts[-1]\n    \n    if label == \"bonafide\" and speaker_id not in speaker_refs:\n        speaker_refs[speaker_id] = audio_filename\n\n# Initialize lists to hold pairs of inputs and their labels, limited to 10,000 audios\nX_pairs = []  # To hold pairs of inputs\ny_pairs = []  # To hold labels for each pair\nprocessed_audios = 0  # Initialize a counter\n\n# Second pass to create input pairs and labels\nfor line in lines:\n    if processed_audios >= 10000:  # Break the loop if the limit is reached\n        break\n\n    parts = line.strip().split()\n    speaker_id, audio_filename, label = parts[0], parts[1], parts[-1]\n    audio_path = os.path.join(DATASET_PATH, audio_filename + \".flac\")\n    \n    if speaker_id in speaker_refs:\n        ref_audio_path = os.path.join(DATASET_PATH, speaker_refs[speaker_id] + \".flac\")\n        test_audio_mfcc = load_and_transform_audio_to_mfcc(audio_path, DURATION, SAMPLE_RATE, N_MFCC)\n        ref_audio_mfcc = load_and_transform_audio_to_mfcc(ref_audio_path, DURATION, SAMPLE_RATE, N_MFCC)\n        \n        if test_audio_mfcc is not None and ref_audio_mfcc is not None:\n            X_pairs.append([ref_audio_mfcc, test_audio_mfcc])\n            y_pairs.append(1 if label == \"bonafide\" else 0)\n            processed_audios += 1\n\n# Convert lists to numpy arrays for the model\nX_pairs = np.array(X_pairs)  # Shape: (num_pairs, 2, n_mfcc, time_steps, 1)\ny_pairs = np.array(y_pairs)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_pairs.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN Model Archeticture Using Siamese Network","metadata":{}},{"cell_type":"code","source":"\ndef build_base_network(input_shape):\n    \"\"\"\n    Defines the base network for feature extraction, updated for MFCC input shape.\n    \"\"\"\n    input = Input(shape=input_shape)\n    x = Conv2D(32, (3, 3), activation='relu')(input)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2))(x)\n    x = Dropout(0.25)(x)\n\n    x = Conv2D(64, (3, 3), activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2))(x)\n    x = Dropout(0.25)(x)\n\n    x = Flatten()(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dense(64, activation='relu')(x)\n    return Model(input, x)\n\ndef euclidean_distance(vects):\n    \"\"\"\n    Computes the Euclidean distance between two vectors.\n    \"\"\"\n    x, y = vects\n    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n\ndef build_siamese_model(input_shape):\n    \"\"\"\n    Constructs the Siamese network architecture, updated for MFCC input shape.\n    \"\"\"\n    left_input = Input(input_shape)\n    right_input = Input(input_shape)\n\n    base_network = build_base_network(input_shape)\n    \n    left_output = base_network(left_input)\n    right_output = base_network(right_input)\n\n    distance = Lambda(euclidean_distance)([left_output, right_output])\n\n    prediction = Dense(1, activation='sigmoid')(distance)\n\n    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n\n    siamese_net.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    return siamese_net\n\n# Assuming your MFCCs have shape (13, 109, 1) based on the error message\ninput_shape = (13, 109, 1)  # Update this based on your actual MFCC shape\nsiamese_model = build_siamese_model(input_shape)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adjust input data for training the model\n","metadata":{}},{"cell_type":"code","source":"# Adjust the input_shape for MFCC\ninput_shape = (13, 109, 1)  # Updated for MFCC\n\n# Make sure the build_siamese_model function is updated to accept this input_shape\nsiamese_model = build_siamese_model(input_shape)\n\n# Assuming X_pairs and y_pairs are structured correctly\nleft_inputs = np.array([pair[0] for pair in X_pairs])\nright_inputs = np.array([pair[1] for pair in X_pairs])\n\n# Ensure left_inputs and right_inputs are reshaped correctly for the model\nleft_inputs = left_inputs.reshape(-1, 13, 109, 1)  # Reshape to match the model's expected input\nright_inputs = right_inputs.reshape(-1, 13, 109, 1)  # Same for the right inputs\n\n# Model training\nsiamese_model.fit([left_inputs, right_inputs], y_pairs, batch_size=32, epochs=10, validation_split=0.2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Assuming build_siamese_model and your data preparation code goes here\n\n# Fit the model and capture the history\nhistory = siamese_model.fit([left_inputs, right_inputs], y_pairs, batch_size=32, epochs=10, validation_split=0.2)\n\n# Extract accuracy and loss values\ntrain_accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(train_accuracy) + 1)\n\n# Accuracy curve\nplt.figure(figsize=(14, 5))\nplt.subplot(1, 2, 1)\nplt.plot(epochs, train_accuracy, 'b-', label='Training Accuracy')\nplt.plot(epochs, val_accuracy, 'r-', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Adjust y-axis to include 100% clearly\naccuracy_padding = (max(train_accuracy + val_accuracy) - min(train_accuracy + val_accuracy)) * 0.4  # Padding\naccuracy_upper_limit = min(1, max(train_accuracy + val_accuracy) + accuracy_padding) * 1.01  # Slightly above 100%\nplt.ylim([max(0, min(train_accuracy + val_accuracy) - accuracy_padding), accuracy_upper_limit])\n\n# Loss curve\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_loss, 'b-', label='Training Loss')\nplt.plot(epochs, val_loss, 'r-', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Adjust loss y-axis similarly, if necessary\nloss_padding = (max(train_loss + val_loss) - min(train_loss + val_loss)) * 0.4  # Padding\nplt.ylim([min(train_loss + val_loss) - loss_padding, max(train_loss + val_loss) + loss_padding])\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test the model","metadata":{}},{"cell_type":"markdown","source":"# Testing Model on ASVspoof 2019 dataset","metadata":{}},{"cell_type":"code","source":"# Assuming siamese_model is your trained Siamese network model\nDATASET_PATH=\"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_dev/flac\"\n\nLABEL_FILE_PATH = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\"\n\n# Function to load and prepare testing pairs from the label file, including bonafide audios for testing\ndef load_testing_pairs(label_file_path, dataset_path, limit=5000):\n    speaker_refs = {}  # Dictionary to hold the first bonafide audio for each speaker\n    testing_pairs = []\n    counter = 0\n\n    try:\n        with open(label_file_path, 'r') as file:\n            lines = file.readlines()\n            for line in lines:\n                if counter >= limit:\n                    break\n                parts = line.strip().split()\n                speaker_id, audio_filename, label = parts[0], parts[1], parts[-1]\n\n                audio_filename_with_extension = audio_filename + \".flac\"\n\n                if label == \"bonafide\":\n                    if speaker_id not in speaker_refs:\n                        # Store the first \"bonafide\" audio filename for each speaker\n                        speaker_refs[speaker_id] = audio_filename_with_extension\n                        # Add this bonafide audio as a testing pair with itself as a reference\n                        testing_pairs.append((speaker_id, audio_filename_with_extension, audio_filename_with_extension, label))\n                    else:\n                        # For additional \"bonafide\" audios, use the first \"bonafide\" audio as reference\n                        ref_audio_filename = speaker_refs[speaker_id]\n                        testing_pairs.append((speaker_id, ref_audio_filename, audio_filename_with_extension, label))\n                elif speaker_id in speaker_refs:\n                    # For \"fake\" audios, find their corresponding reference \"bonafide\" audio\n                    ref_audio_filename = speaker_refs[speaker_id]\n                    testing_pairs.append((speaker_id, ref_audio_filename, audio_filename_with_extension, label))\n\n                counter += 1\n\n    except IOError:\n        print(f\"Could not read file: {label_file_path}\")\n\n    return testing_pairs\n\n# Load testing pairs with a limit of 5000 audios\ntesting_pairs = load_testing_pairs(LABEL_FILE_PATH, DATASET_PATH, limit=5000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Function to process and prepare pairs for testing\ndef prepare_test_pairs(testing_pairs, dataset_path, sample_rate, duration, n_mfcc, n_fft, hop_length, time_steps):\n    X_test_pairs = []\n    y_test_labels = []\n    \n    for speaker_id, reference_audio, test_audio, label in testing_pairs:\n        # Remove the additional \".flac\" since it's already included in the file names\n        ref_audio_path = os.path.join(dataset_path, reference_audio)  # Assuming '.flac' is already in reference_audio\n        test_audio_path = os.path.join(dataset_path, test_audio)  # Assuming '.flac' is already in test_audio\n        \n        ref_mfcc_db = load_and_transform_audio_to_mfcc(ref_audio_path, duration, sample_rate, n_mfcc, n_fft, hop_length, time_steps)\n        test_mfcc_db = load_and_transform_audio_to_mfcc(test_audio_path, duration, sample_rate, n_mfcc, n_fft, hop_length, time_steps)\n        \n        if ref_mfcc_db is not None and test_mfcc_db is not None:\n            X_test_pairs.append([ref_mfcc_db, test_mfcc_db])\n            y_test_labels.append(1 if label == \"bonafide\" else 0)\n    \n    return np.array(X_test_pairs), np.array(y_test_labels)\n\n# Prepare test pairs\nX_test_pairs, y_test_labels = prepare_test_pairs(testing_pairs, DATASET_PATH, SAMPLE_RATE, DURATION, N_MFCC, n_fft=2048, hop_length=512, time_steps=109)\n\n# Reshape test pairs for the model\nleft_inputs = np.array([pair[0] for pair in X_test_pairs]).reshape(-1, N_MFCC, 109, 1)\nright_inputs = np.array([pair[1] for pair in X_test_pairs]).reshape(-1, N_MFCC, 109, 1)\n\n# Testing the model\npredictions = siamese_model.predict([left_inputs, right_inputs])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, roc_curve, auc, f1_score\nimport matplotlib.pyplot as plt\n\n# Convert predictions to binary outcomes\nbinary_predictions = [1 if pred >= 0.5 else 0 for pred in predictions.ravel()]\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test_labels, binary_predictions)*100\n\nprint(f\"Accuracy: {accuracy:.4f} %\")\n\n# Calculate F1 score\nf1 = f1_score(y_test_labels, binary_predictions)*100\nprint(f\"F1 Score: {f1:.4f} %\" )\n\n# Compute ROC curve and ROC area\nfpr, tpr, thresholds = roc_curve(y_test_labels, predictions.ravel())\nroc_auc = auc(fpr, tpr)\n\n# Plot ROC curve\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, confusion_matrix\n\n# Calculate precision and recall\nprecision = precision_score(y_test_labels, binary_predictions) * 100\nrecall = recall_score(y_test_labels, binary_predictions) * 100\n\n# Calculate specificity\ntn, fp, _, _ = confusion_matrix(y_test_labels, binary_predictions).ravel()\nspecificity = (tn / (tn + fp)) * 100\n\nprint(f\"Precision: {precision:.4f} %\")\nprint(f\"Recall: {recall:.4f} %\")\nprint(f\"Specificity: {specificity:.4f} %\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming binary_predictions are available from the previous step\n\n# Generate the confusion matrix\ncm = confusion_matrix(y_test_labels, binary_predictions)\n\n# Plotting the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Model on Deep-voice dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport librosa\nreal_audio_dir=\"/kaggle/input/deep-voice-deepfake-voice-recognition/KAGGLE/AUDIO/REAL\"\n\nfake_audio_dir = \"/kaggle/input/deep-voice-deepfake-voice-recognition/KAGGLE/AUDIO/FAKE\"\ndef load_and_transform_audio_to_mfcc(filepath, target_duration, sample_rate, n_mfcc, n_fft=2048, hop_length=512, time_steps=109):\n    try:\n        audio, sr = librosa.load(filepath, sr=sample_rate, duration=target_duration)\n        # Apply noise reduction\n        reduced_noise_audio = librosa.effects.preemphasis(audio)\n        mfccs = librosa.feature.mfcc(y=reduced_noise_audio, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n        \n        if mfccs.shape[1] < time_steps:\n            mfccs = np.pad(mfccs, ((0, 0), (0, time_steps - mfccs.shape[1])), 'constant')\n        elif mfccs.shape[1] > time_steps:\n            mfccs = mfccs[:, :time_steps]\n        \n        return mfccs.reshape((n_mfcc, time_steps, 1))\n    except Exception as e:\n        print(f\"Error processing {filepath}: {str(e)}\")\n        return None\n\n# Assuming `real_audio_dir` and `fake_audio_dir` are defined and contain your audio files\nreal_audio_files = [os.path.join(real_audio_dir, file) for file in os.listdir(real_audio_dir) if file.endswith(\".wav\")]\nfake_audio_files = [os.path.join(fake_audio_dir, file) for file in os.listdir(fake_audio_dir) if file.endswith(\".wav\")]\n\nSAMPLE_RATE = 16000\nDURATION = 5\nN_MELS = 13\ntime_steps = 109\n\nall_predictions = []\nall_true_labels = []\npair_info = []\n\n# Helper function to extract prefix\ndef get_prefix(filename):\n    \n    return os.path.basename(filename).lower().split('-')[0]\n\n# Compare each real audio with itself and its corresponding fake audio\nfor real_audio_path in real_audio_files:\n    real_prefix = get_prefix(real_audio_path)\n    \n    # Compare with itself\n    test_audio_paths = [real_audio_path] + [fake_path for fake_path in fake_audio_files if get_prefix(fake_path) == real_prefix]\n    \n    for test_audio_path in test_audio_paths:\n        is_real_pair = real_audio_path == test_audio_path\n        \n        ref_mfcc_db = load_and_transform_audio_to_mfcc(real_audio_path, DURATION, SAMPLE_RATE, N_MELS, n_fft=2048, hop_length=512, time_steps=time_steps)\n        test_mfcc_db = load_and_transform_audio_to_mfcc(test_audio_path, DURATION, SAMPLE_RATE, N_MELS, n_fft=2048, hop_length=512, time_steps=time_steps)\n        \n        if ref_mfcc_db is not None and test_mfcc_db is not None:\n            ref_mfcc_db_reshaped = ref_mfcc_db.reshape(1, N_MELS, time_steps, 1)\n            test_mfcc_db_reshaped = test_mfcc_db.reshape(1, N_MELS, time_steps, 1)\n            prediction = siamese_model.predict([ref_mfcc_db_reshaped, test_mfcc_db_reshaped])[0][0]\n            \n            all_predictions.append(prediction)\n            all_true_labels.append(1 if is_real_pair else 0)  # 1 for real, 0 for fake\n            pair_info.append((real_audio_path, test_audio_path))\n\n# Proceed to evaluate the predictions with your preferred metrics\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert predictions to binary using 0.5 threshold\npredicted_labels = [1 if pred >= 0.5 else 0 for pred in all_predictions]\n\n# True labels should be in the correct binary format (0s and 1s), ensure this before proceeding\ntrue_labels = all_true_labels\n\n# Calculate accuracy and F1 score\naccuracy = accuracy_score(true_labels, predicted_labels)*100\n\nprint(f\"Accuracy: {accuracy:.4f} %\")\n\n# Generate confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n# Calculate ROC Curve and AUC\nfpr, tpr, thresholds = roc_curve(true_labels, predicted_labels)\nroc_auc = auc(fpr, tpr)\n# Plotting the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, auc, roc_curve\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assuming `all_predictions` and `all_true_labels` are defined and valid\n# Your existing code for predictions, accuracy, and confusion matrix goes here\n\n# Calculate precision, recall, and F1 score\nprecision = precision_score(true_labels, predicted_labels) * 100\nrecall = recall_score(true_labels, predicted_labels) * 100\nf1 = f1_score(true_labels, predicted_labels) * 100\n\n# Calculate specificity from the confusion matrix\ntn, fp, fn, tp = conf_matrix.ravel()\nspecificity = (tn / (tn + fp)) * 100\n\nprint(f\"Precision: {precision:.4f} %\")\nprint(f\"Recall: {recall:.4f} %\")\nprint(f\"Specificity: {specificity:.4f} %\")\nprint(f\"F1 Score: {f1:.4f} %\")\n\n# Plot ROC Curve\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_and_transform_audio_to_mfcc(filepath, target_duration, sample_rate, n_mfcc, n_fft=2048, hop_length=512, time_steps=109):\n    try:\n        audio, sr = librosa.load(filepath, sr=sample_rate, duration=target_duration)\n        \n        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n        \n        if mfccs.shape[1] < time_steps:\n            mfccs = np.pad(mfccs, ((0, 0), (0, time_steps - mfccs.shape[1])), 'constant')\n        elif mfccs.shape[1] > time_steps:\n            mfccs = mfccs[:, :time_steps]\n        \n        return mfccs.reshape((n_mfcc, time_steps, 1))\n    except Exception as e:\n        print(f\"Error processing {filepath}: {str(e)}\")\n        return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Model on ASVspoof2021 dataset","metadata":{}},{"cell_type":"code","source":"# Assuming siamese_model is your trained Siamese network model\nDATASET_PATH=\"/kaggle/input/asvspoof-21-df/ASVspoof2021_DF_eval_part00/ASVspoof2021_DF_eval/flac\"\nLABEL_FILE_PATH = \"/kaggle/input/metadata/trial_metadata.txt\"\nSAMPLE_RATE = 16000\nDURATION = 5\nN_MFCC = 13  \n\n# Function to load and prepare testing pairs from the label file, including bonafide audios for testing\ndef load_testing_pairs(label_file_path, dataset_path, limit=2000):\n    speaker_refs = {}  # Dictionary to hold the first bonafide audio for each speaker\n    testing_pairs = []\n    counter = 0\n\n    try:\n        with open(label_file_path, 'r') as file:\n            lines = file.readlines()\n            for line in lines:\n                if counter >= limit:\n                    break\n                parts = line.strip().split()\n                speaker_id, audio_filename, label = parts[0], parts[1], parts[5]\n\n                audio_filename_with_extension = audio_filename + \".flac\"\n\n                if label == \"bonafide\":\n                    if speaker_id not in speaker_refs:\n                        # Store the first \"bonafide\" audio filename for each speaker\n                        speaker_refs[speaker_id] = audio_filename_with_extension\n                        # Add this bonafide audio as a testing pair with itself as a reference\n                        testing_pairs.append((speaker_id, audio_filename_with_extension, audio_filename_with_extension, label))\n                    else:\n                        # For additional \"bonafide\" audios, use the first \"bonafide\" audio as reference\n                        ref_audio_filename = speaker_refs[speaker_id]\n                        testing_pairs.append((speaker_id, ref_audio_filename, audio_filename_with_extension, label))\n                elif speaker_id in speaker_refs:\n                    # For \"fake\" audios, find their corresponding reference \"bonafide\" audio\n                    ref_audio_filename = speaker_refs[speaker_id]\n                    testing_pairs.append((speaker_id, ref_audio_filename, audio_filename_with_extension, label))\n\n                counter += 1\n\n    except IOError:\n        print(f\"Could not read file: {label_file_path}\")\n\n    return testing_pairs\n\n# Load testing pairs with a limit of 5000 audios\ntesting_pairs = load_testing_pairs(LABEL_FILE_PATH, DATASET_PATH, limit=2000)\n# Function to process and prepare pairs for testing\ndef prepare_test_pairs(testing_pairs, dataset_path, sample_rate, duration, n_mfcc, n_fft, hop_length, time_steps):\n    X_test_pairs = []\n    y_test_labels = []\n    \n    for speaker_id, reference_audio, test_audio, label in testing_pairs:\n        # Remove the additional \".flac\" since it's already included in the file names\n        ref_audio_path = os.path.join(dataset_path, reference_audio)  # Assuming '.flac' is already in reference_audio\n        test_audio_path = os.path.join(dataset_path, test_audio)  # Assuming '.flac' is already in test_audio\n        \n        ref_mfcc_db = load_and_transform_audio_to_mfcc(ref_audio_path, duration, sample_rate, n_mfcc, n_fft, hop_length, time_steps)\n        test_mfcc_db = load_and_transform_audio_to_mfcc(test_audio_path, duration, sample_rate, n_mfcc, n_fft, hop_length, time_steps)\n        \n        if ref_mfcc_db is not None and test_mfcc_db is not None:\n            X_test_pairs.append([ref_mfcc_db, test_mfcc_db])\n            y_test_labels.append(1 if label == \"bonafide\" else 0)\n    \n    return np.array(X_test_pairs), np.array(y_test_labels)\n\n# Prepare test pairs\nX_test_pairs, y_test_labels = prepare_test_pairs(testing_pairs, DATASET_PATH, SAMPLE_RATE, DURATION, N_MFCC, n_fft=2048, hop_length=512, time_steps=109)\n\n# Reshape test pairs for the model\nleft_inputs = np.array([pair[0] for pair in X_test_pairs]).reshape(-1, N_MFCC, 109, 1)\nright_inputs = np.array([pair[1] for pair in X_test_pairs]).reshape(-1, N_MFCC, 109, 1)\n\n# Testing the model\npredictions = siamese_model.predict([left_inputs, right_inputs])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, roc_curve, auc, f1_score\nimport matplotlib.pyplot as plt\n\n# Convert predictions to binary outcomes\nbinary_predictions = [1 if pred >= 0.5 else 0 for pred in predictions.ravel()]\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test_labels, binary_predictions)*100\n\nprint(f\"Accuracy: {accuracy:.4f} %\")\n\n# Calculate F1 score\nf1 = f1_score(y_test_labels, binary_predictions)*100\nprint(f\"F1 Score: {f1:.4f} %\" )\n\n# Compute ROC curve and ROC area\nfpr, tpr, thresholds = roc_curve(y_test_labels, predictions.ravel())\nroc_auc = auc(fpr, tpr)\n\n# Plot ROC curve\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, confusion_matrix\n\n# Calculate precision and recall\nprecision = precision_score(y_test_labels, binary_predictions) * 100\nrecall = recall_score(y_test_labels, binary_predictions) * 100\n\n# Calculate specificity\ntn, fp, _, _ = confusion_matrix(y_test_labels, binary_predictions).ravel()\nspecificity = (tn / (tn + fp)) * 100\n\nprint(f\"Precision: {precision:.4f} %\")\nprint(f\"Recall: {recall:.4f} %\")\nprint(f\"Specificity: {specificity:.4f} %\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming binary_predictions are available from the previous step\n\n# Generate the confusion matrix\ncm = confusion_matrix(y_test_labels, binary_predictions)\n\n# Plotting the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Model to Export","metadata":{}},{"cell_type":"code","source":"siamese_model.save_weights(\"siamese_model.weights.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}